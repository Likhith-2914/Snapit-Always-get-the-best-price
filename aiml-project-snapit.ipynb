{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AI-511 Machine Learning Project 2022-23","metadata":{}},{"cell_type":"markdown","source":"## Imports for the Notebook","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import sqrt\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport re\nimport string\nimport time\nimport random\nfrom random import uniform\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection._univariate_selection import SelectKBest, f_regression\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nimport xgboost as xgb\nfrom sklearn.svm import SVR\nimport lightgbm as LGBM\nfrom sklearn.neural_network import MLPRegressor\n\n\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse import hstack\nfrom scipy.sparse import vstack\nfrom scipy.stats import randint as sp_randint\n\n\n\nSTOPWORDS = set(stopwords.words('english'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the Dataset","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/snapit-always-get-the-best-price/train.csv\")\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/snapit-always-get-the-best-price/test.csv\")\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Preprocessing the Data","metadata":{}},{"cell_type":"markdown","source":"### 1.1 Partitioning `CATEGORY` column\n\nWe can observe that a data point's `CATEGORY` is of the form `CATEGORY_1/CATEGORY_2/.../CATEGORY_5`. For further application of models, we divide the feature into 5 sub-divisions.","metadata":{}},{"cell_type":"code","source":"def divideCategory(df):\n    df[['CATEGORY_1','CATEGORY_2','CATEGORY_3','CATEGORY_4','CATEGORY_5']] = df.CATEGORY.str.split(\"/\",expand=True)\n\ndef dropCol(df, col):\n    df.drop(axis = \"columns\", labels=col, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"divideCategory(train_df)\ndropCol(train_df, \"CATEGORY\")\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"divideCategory(test_df)\ndropCol(test_df, \"CATEGORY\")\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Neutralise the Null values\n\nNULL values occur whenever there is no value given for a data point's particular feature. We must either eliminate/replace these null values.","metadata":{}},{"cell_type":"code","source":"# null values of each column\ntrain_df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def replaceNulls(df, col, const):\n    df[col].fillna(const, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_valued_columns = [\"PRODUCT_BRAND\", \"PRODUCT_DESCRIPTION\", \"CATEGORY_1\", \"CATEGORY_2\", \"CATEGORY_3\", \"CATEGORY_4\", \"CATEGORY_5\"]\nfor column in null_valued_columns:\n    replaceNulls(train_df, column, \"none\")\n\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, there are no more NULLs left. The process is same for test data","metadata":{}},{"cell_type":"code","source":"test_df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Replace the null values","metadata":{}},{"cell_type":"code","source":"# they are the same as in train_df\nfor column in null_valued_columns:\n    replaceNulls(test_df, column, \"none\")\n\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 Useful Feature Extraction\n\nThis section removes the unwanted features from the data based on their correlation with the target feature","metadata":{}},{"cell_type":"markdown","source":"Pearson Coeffecient is used to calculate correlation between two \"continous\" features. The only continous features in the data are **PRODUCT_ID** and **PRODUCT_PRICE**.","metadata":{}},{"cell_type":"code","source":"train_df[[\"PRODUCT_ID\", \"PRODUCT_PRICE\"]].corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that correlation of **PRODUCT_ID** with **PRODUCT_PRICRE** is very low. Thus, this column does not affect much the data. So, we can remove the feature. ","metadata":{}},{"cell_type":"code","source":"dropCol(train_df, \"PRODUCT_ID\")\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saved for documenting predicted product prices\ntest_product_id = test_df[\"PRODUCT_ID\"]\ndropCol(test_df, \"PRODUCT_ID\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.4 Preprocess text features","metadata":{}},{"cell_type":"markdown","source":"**Text Features**: Features where the values are textual (typically not a single word). In our case `PRODUCT_NAME` and `PRODUCT_DESCRIPTION`. <br>\n<br>\nTo apply any model, all the features must be in numerical form. We use NLP(Natural Language Processing) to convert these Text features into Numerical features. For a better performance, we do a small pre-processinf to these text features","metadata":{}},{"cell_type":"code","source":"# replaces the general english shortcuts with their full forms\ndef replace_shortcuts(sentence):\n\n    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n    sentence = re.sub(r\"won't\", \"will not\", sentence)\n    sentence = re.sub(r\"can\\'t\", \"can not\", sentence)\n        \n    return sentence\n\n# removes the punctuation marks\ndef remove_punctuation(sentence):\n    \n    regular_punct = list(string.punctuation)\n    for punc in regular_punct:\n        if punc in sentence:\n            sentence = sentence.replace(punc, ' ')\n\n    return sentence.strip()\n\n# remove (if any) emoji's (like :) )\ndef remove_emoji(sentence):\n    \n    pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"\n                           u\"\\U0001F300-\\U0001F5FF\"\n                           u\"\\U0001F680-\\U0001F6FF\"\n                           u\"\\U0001F1E0-\\U0001F1FF\"\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    \n    return pattern.sub(r'', sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pre processing Text features\ndef preprocess_text_feature(feature):\n    processed_feature = []\n    for sentence in feature.values:\n            sent = replace_shortcuts(sentence)\n            sent = sent.replace('\\\\r', ' ')\n            sent = sent.replace('\\\\\"', ' ')\n            sent = sent.replace('\\\\n', ' ')\n            sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n            sent = remove_emoji(sent)\n            sent = remove_punctuation(sent)\n            sent = ' '.join(e for e in sent.split() if e not in STOPWORDS)\n            processed_feature.append(sent.lower().strip())\n    return processed_feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_features = [\"PRODUCT_NAME\", \"PRODUCT_DESCRIPTION\"]\nfor feature in text_features:\n    train_df[feature] = preprocess_text_feature(train_df[feature])\n    test_df[feature] = preprocess_text_feature(test_df[feature])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.5 Split for CROSS-VALIDATION\n\nCross-validation is a techinnque used to verify the goodness of a model. We can tune the hyper parameters and assess the effeciency of the model by testing on the cross-validated (cv_df).","metadata":{}},{"cell_type":"code","source":"# log1p(x) = log(x+1): useful while calculating RMSLE\ny_tr = np.log1p(train_df[\"PRODUCT_PRICE\"])\ntrain_df, cv_df , y_train, y_cv = train_test_split(train_df, y_tr, test_size=0.1, random_state=42)\ncv_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.6 Categorical to Numerical","metadata":{}},{"cell_type":"markdown","source":"In this section, we convert all categorical features into numerical features <br>\nFor `CATEGORY_i`, `PRODUCT_BRAND`: `CountVecotrizer()` - Numeric Encoder<br>\nFor `PRODUCT_NAME`. `PRODUCT_DESCRIPTION`: `TfidfVectorizer()` - Text Encoder","metadata":{}},{"cell_type":"code","source":"train_set = {}\ncv_set = {}\ntest_set = {}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextEncoder:\n\n    def __init__(self, train_df, cv_df, test_df):\n        self.train_df = train_df\n        self.test_df = test_df\n        self.cv_df = cv_df\n    \n    def TextEncode(self, column):\n        tfidfvectorizer = TfidfVectorizer()\n        train_te = tfidfvectorizer.fit_transform(self.train_df[column])\n        cv_te = tfidfvectorizer.transform(self.cv_df[column])\n        test_te = tfidfvectorizer.transform(self.test_df[column])\n        return (train_te, cv_te, test_te)\n\nfeatures_to_te = [\"PRODUCT_NAME\", \"PRODUCT_DESCRIPTION\"]\nTextEncoder = TextEncoder(train_df, cv_df, test_df)\nfor feature in features_to_te:\n    (train_set[feature], cv_set[feature], test_set[feature]) = TextEncoder.TextEncode(feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OneHotEncoder:\n    \n    def __init__(self, train_df, cv_df, test_df):\n        self.train_df = train_df\n        self.test_df = test_df\n        self.cv_df = cv_df\n\n    def OneHotEncode(self, column):\n        countvectorizer = CountVectorizer()\n        train_ohe = countvectorizer.fit_transform(self.train_df[column])\n        cv_ohe = countvectorizer.transform(self.cv_df[column])\n        test_ohe = countvectorizer.transform(self.test_df[column])\n        return (train_ohe, cv_ohe, test_ohe)\n\nfeatures_to_ohe = [\"CATEGORY_1\", \"CATEGORY_2\", \"CATEGORY_3\", \"CATEGORY_4\", \"CATEGORY_5\", \"PRODUCT_BRAND\"]\nOneHotEncoder = OneHotEncoder(train_df, cv_df, test_df)\nfor feature in features_to_ohe:\n    (train_set[feature], cv_set[feature], test_set[feature]) = OneHotEncoder.OneHotEncode(feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"other_features = [\"PRODUCT_CONDITION\", \"SHIPPING_AVAILABILITY\"]\nfor feature in other_features:\n    train_set[feature] = csr_matrix(pd.get_dummies(train_df[feature], sparse=True).values)\n    cv_set[feature] = csr_matrix(pd.get_dummies(cv_df[feature], sparse=True).values)\n    test_set[feature] = csr_matrix(pd.get_dummies(test_df[feature], sparse=True).values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Merging all the csr matrices into a single csr matrix","metadata":{}},{"cell_type":"code","source":"from scipy.sparse import hstack\ndef mergeAll(set):\n    tup = tuple(list(set.values()))\n    merged = hstack(tup).tocsr().astype('float32')\n    return merged","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = mergeAll(train_set)\nX_cv = mergeAll(cv_set)\nX_test = mergeAll(test_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.7 Normalisation","metadata":{}},{"cell_type":"code","source":"# class Normalizer:\n    \n#     def __init__(self, feature):\n#         self.feature = feature\n#         self.min = feature.min()\n#         self.max = feature.max()\n\n#     def normalize(self):\n#         normalized_feature = (self.feature - self.min)/(self.max - self.min)\n#         return normalized_feature\n    \n#     def denormalize(self, normalized_feature):\n#         denormalized_feature = (normalized_feature * (self.max - self.min)) + self.min\n#         return denormalized_feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TrainPriceNormalizer = Normalizer(y_train)\n# y_train = TrainPriceNormalizer.normalize()\n\n# CVPriceNormalizer = Normalizer(y_cv)\n# y_cv = CVPriceNormalizer.normalize()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_train = TrainPriceNormali/zer.denormalize(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.8 Outlier Detection and Removal\n\nThis section of Preprocessing checks for outliers and remove them (if present) to avoid misleading the model","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x = train_df[\"PRODUCT_CONDITION\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we observe the above boxplot, there are no points outside the whiskers. Hence, there are no outlier points for the PRODUCT_CONDITION column. By checking for every feature in this way, we found no outliers. ","metadata":{}},{"cell_type":"markdown","source":"## 2. Regression Models\n\nBy now we had completely pre-processed train, val and test data. We will be training different Regression models using train data and test them across Val data to assess the model. Then we predict the price of test data for the final submission","metadata":{}},{"cell_type":"code","source":"def convertToCSV(product_price, product_id, filename):\n    test_product_id = list(product_id)\n    test_predicted_price = list(product_price)\n\n    data = {'PRODUCT_ID': product_id,\n        'PRODUCT_PRICE': product_price}\n\n    pd.DataFrame(data).to_csv(filename+\".csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_cv = vstack((X_train, X_cv)).tocsr().astype('float32')\ny_train_cv = list(y_train) + list(y_cv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1 Linear Regression\n\nFirst, we use the naive Linear Regression Model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nstart = time.time()\nLR_model = LinearRegression()\nLR_model.fit(X_train_cv, y_train_cv)\nend = time.time()\n\nduration = round(end-start, 2)\nprint(\"Training time: \" + str(duration) + \" secs\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_test_y_LR = LR_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Ridge Regression","metadata":{}},{"cell_type":"markdown","source":"In this section, we apply Ridge Regression model on our data. Initially, we tune the hyperparam alpha for a best model using cross-validation. The method of error-check used is RMSLE","metadata":{}},{"cell_type":"code","source":"# cross-validation for best alpha\n\nalpha = [1.7, 2.1, 2.3, 3] \ncv_rmsle_array=[]\nfor i in alpha:\n    model = Ridge(solver=\"sag\", random_state=42, alpha=i)\n    model.fit(X_train_cv, y_train_cv)\n    y_pred_cv = model.predict(X_cv)\n    cv_rmsle_array.append(sqrt(mse(y_pred_cv, y_cv)))\n\nbest_alpha = np.argmin(cv_rmsle_array)\nprint(\"Best value for Alpha = \" + str(alpha[best_alpha]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nstart = time.time()\nRR_model = Ridge(solver = \"sag\", random_state =42, alpha = alpha[best_alpha])\nRR_model.fit(X_train_cv, y_train_cv)\nend = time.time()\n\nduration = round(end-start, 2)\nprint(\"Training time: \" + str(duration) + \" secs\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_cv_pred_RR = RR_model.predict(X_train_cv)\ny_train_pred_RR = RR_model.predict(X_train)\ny_cv_pred_RR = RR_model.predict(X_cv)\ny_test_pred_RR = RR_model.predict(X_test)\n\nprint(\"Error in Train+CV: \" + str(sqrt(mse(y_train_cv, y_train_cv_pred_RR))))\nprint(\"Error in Train: \" + str(sqrt(mse(y_train, y_train_pred_RR))))\nprint(\"Error in CV: \" + str(sqrt(mse(y_cv, y_cv_pred_RR))))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"convertToCSV(np.expm1(y_test_pred_RR), test_product_id, \"output_RR\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 XGBoost","metadata":{}},{"cell_type":"code","source":"start = time.time()\nXGBR_model = xgb.XGBRegressor(n_estimators=100, eta=0.8)\nXGBR_model.fit(X_train_cv,y_train_cv)\nend = time.time()\n\nduration = round((end-start)/60.0, 2)\nprint(\"Training time: \" + str(duration) + \"mins\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_cv_XGB_pred = XGBR_model.predict(X_train_cv)\ny_train_XGB_pred = XGBR_model.predict(X_train)\ny_cv_XGB_pred = XGBR_model.predict(X_cv)\ny_test_XGB_pred = XGBR_model.predict(X_test)\n\nprint(\"TrainCV: \" + str(sqrt(mse(y_train_cv, y_train_cv_XGB_pred))))\nprint(\"Train: \" + str(sqrt(mse(y_train, y_train_XGB_pred))))\nprint(\"CV: \" + str(sqrt(mse(y_cv, y_cv_XGB_pred))))\ny_test_XGB_final = np.expm1(y_test_XGB_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_product_id = list(test_product_id)\ntest_predicted_price = list(y_test_XGB_final)\n\ndata = {'PRODUCT_ID': test_product_id,\n    'PRODUCT_PRICE': test_predicted_price}\n\npd.DataFrame(data).to_csv(\"output_XGB.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Extracting k-important features\nComing models can't take all features..so we extratc imp features","metadata":{}},{"cell_type":"code","source":"numerical_keys = [\"PRODUCT_CONDITION\", \"SHIPPING_AVAILABILITY\"]\nnon_numeric_train = hstack(tuple([value for key, value in train_set.items() if key not in numerical_keys]))\nnon_numeric_cv    = hstack(tuple([value for key, value in cv_set.items() if key not in numerical_keys]))\nnon_numeric_test  = hstack(tuple([value for key, value in test_set.items() if key not in numerical_keys]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_features = 10000\nfeature_selector = SelectKBest(f_regression, k=max_features)\n\nnon_numeric_train_features = feature_selector.fit_transform(non_numeric_train, y_train)\nnon_numeric_cv_features    = feature_selector.transform(non_numeric_cv)\nnon_numeric_test_features  = feature_selector.transform(non_numeric_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = hstack((non_numeric_train_features, train_set[\"PRODUCT_CONDITION\"], train_set[\"SHIPPING_AVAILABILITY\"]))\nX_cv    = hstack((non_numeric_cv_features, cv_set[\"PRODUCT_CONDITION\"], cv_set[\"SHIPPING_AVAILABILITY\"]))\nX_test  = hstack((non_numeric_test_features, test_set[\"PRODUCT_CONDITION\"], test_set[\"SHIPPING_AVAILABILITY\"]))\nX_train_cv = vstack((X_train, X_cv))\nX_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5 SVM","metadata":{}},{"cell_type":"code","source":"\nstart = time.time()\nSVR_model = SVR(C=3, max_iter=200)\nSVR_model.fit(X_train, y_train)\nend = time.time()\n\nduration = round((end-start)/60.0, 2)\nprint(\"Training time: \" + str(duration) + \"mins\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred_SVR = SVR_model.predict(X_train)\ny_cv_pred_SVR = SVR_model.predict(X_cv)\ny_test_pred_SVR = SVR_model.predict(X_test)\n\nprint(\"Error in Train: \" + str(sqrt(mse(y_train, y_train_pred_SVR))))\nprint(\"Error in CV: \" + str(sqrt(mse(y_cv, y_cv_pred_SVR))))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(y_train_cv)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5 LightGBM","metadata":{}},{"cell_type":"code","source":"import lightgbm as LGBM\n\nLGBR_model = LGBM.LGBMRegressor(learning_rate= 0.7, num_leaves = 30, n_estimators = 800,\n                           min_child_samples = 20, subsample = 0.6, colsample_bytree = 0.6,  \n                           n_jobs=-1 , silent = False)\n\nstart = time.time()\nLGBR_model.fit(X_train_cv, y_train_cv)\nend = time.time()\n\nduration = round((end-start)/60.0, 2)\nprint(\"Training time: \" + str(duration) + \"mins\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_cv_pred_LGBR = LGBR_model.predict(X_train_cv)\ny_train_pred_LGBR = LGBR_model.predict(X_train)\ny_cv_pred_LGBR = LGBR_model.predict(X_cv)\ny_test_pred_LGBR = LGBR_model.predict(X_test)\n\nprint(\"Error in Train-CV: \" + str(sqrt(mse(y_train_cv, y_train_cv_pred_LGBR))))\nprint(\"Error in Train: \" + str(sqrt(mse(y_train, y_train_pred_LGBR))))\nprint(\"Error in CV: \" + str(sqrt(mse(y_cv, y_cv_pred_LGBR))))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"convertToCSV(np.expm1(y_test_pred_LGBR), test_product_id, \"output_LGBR\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.6 MLP","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPRegressor\n\nstart = time.time()\nMLPR_model = MLPRegressor(random_state=1, max_iter=500D).fit(X_train_cv, y_train_cv)\nend = time.time()\n\nduration = round((end-start)/60.0, 2)\nprint(\"Training time: \" + str(duration) + \"mins\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_cv_pred_MLPR = MLPR_model.predict(X_train_cv)\ny_train_pred_MLPR = MLPR_model.predict(X_train)\ny_cv_pred_MLPR    = MLPR_model.predict(X_cv)\ny_test_pred_MLPR  = MLPR_model.predict(X_test)\n\nprint(\"Train-CV Error: \" + str(sqrt(mse(y_train_cv, y_train_cv_pred_MLPR))))\nprint(\"Train Error: \" + str(sqrt(mse(y_train, y_train_pred_MLPR))))\nprint(\"CV: \" + str(sqrt(mse(y_cv, y_cv_pred_MLPR))))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"convertToCSV(np.expm1(y_test_pred_MLPR), test_product_id, \"output_MLP\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.8 Lasso Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nstart = time.time()\nLaR_model = Lasso(alpha = 1)\nLaR_model.fit(X_train_cv, y_train_cv)\nend = time.time()\n\nduration = round((end-start)/60.0, 2)\nprint(\"Training time: \" + str(duration) + \"mins\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By observing on various models, given the resources of GPU, we observed best accuracy(F1-score) of **0.7** for the LightGBM model","metadata":{}},{"cell_type":"markdown","source":"*TEAM: <br>\nN V Sai Likhith - IMT2020118 <br>\nT Akhil - IMT2020124*","metadata":{}}]}